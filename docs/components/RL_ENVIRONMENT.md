# RL Environment Component

## Overview
The **RL Environment** (`src/rl/environment.py`) wraps the entire fuzzing process in a Gymnasium interface, allowing the Reinforcement Learning agent (PPO) to interact with the system.

It translates the complex state of the hybrid fuzzer into a normalized observation vector and defines the reward function that guides the agent toward finding more vulnerabilities efficiently.

## Key Features

### 1. Action Space
The environment exposes a discrete action space with 2 actions:
1.  **FUZZING (0)**: Continue running AFL++ coverage-guided fuzzing. This is the default "exploration" mode.
2.  **SYMBOLIC_EXECUTION (1)**: Pause fuzzing and trigger the `SymbolicExecutor` on a specific target. This is the "breakthrough" mode used to bypass complex checks.

*Note: Resource allocation (CPU/RAM) is handled automatically within these actions based on the `ResourceController`.*

### 2. Observation Space (12-Dimensional)
The agent receives a normalized vector (values 0.0-1.0) representing the system state:

| Index | Feature | Description |
| :--- | :--- | :--- |
| 0 | `paths_normalized` | Total paths found (normalized). |
| 1 | `crashes_normalized` | Total crashes found (normalized). |
| 2 | `exec_speed` | Fuzzer execution speed. |
| 3 | `time_elapsed` | Campaign progress. |
| 4 | `episode_progress` | Steps taken in current episode. |
| 5 | `path_discovery_rate` | Paths found per minute. |
| 6 | `stuck_counter` | How long since last new path. |
| 7 | `fuzzing_recency` | Steps since last fuzzing action. |
| 8 | `coverage_percentage` | Code coverage %. |
| 9 | `symex_roi` | Return on Investment for symbolic execution. |
| 10 | `execs_since_last_path` | **Critical**: Adaptive stuck metric (0.0=fresh, 1.0=stuck). |
| 11 | `target_vuln_score` | Semantic vulnerability score of current target. |

### 3. Reward Function
The reward function is carefully shaped to prioritize crash discovery while encouraging exploration:

- **Crashes**: `+10.0` per new unique crash (Primary Goal).
- **Paths**: `+0.5` per new path (Exploration).
- **Unstuck Bonus**: `+20.0` if symbolic execution successfully finds a new path after the fuzzer was stuck.
- **SymEx Attribution**: `+20.0` if a crash is traced back to a seed generated by symbolic execution.
- **Penalties**:
    - `-5.0` for triggering symbolic execution when NOT stuck (waste of resources).
    - `-1.0` to `-10.0` for repeated symbolic execution failures (encourages backoff).

### 4. Safety & Stability Mechanisms
To prevent the agent from learning degenerate policies (e.g., spamming symbolic execution until the server crashes), the environment implements **Action Masking**:

`_should_block_symex(action)` returns `True` (forcing Fuzzing) if:
1.  **Bootstrap Phase**: First 5 steps of a campaign (AFL needs time to build a corpus).
2.  **Backoff**: The agent is in a "penalty box" due to repeated symex failures.
3.  **Low ROI**: Symbolic execution hasn't yielded results recently.
4.  **Resource Overload**: System CPU/RAM usage is too high.

## Technical Implementation

### Initialization
```python
env = NeuroFuzzEnv(orchestrator, targets, config)
```
It connects to the `Orchestrator` to execute actions and retrieve stats.

### Step Logic
```python
obs, reward, terminated, truncated, info = env.step(action)
```
1.  **Action Masking**: Checks `_should_block_symex`. If blocked, overrides action to `FUZZING`.
2.  **Execution**: Calls `orchestrator.execute_fuzzing_step()` or `orchestrator.execute_symbolic_execution()`.
3.  **Stats Collection**: Retrieves fresh stats from AFL++ and the filesystem.
4.  **Reward Calculation**: Computes reward based on the delta in paths/crashes.
5.  **Observation Update**: Generates the new 12D state vector.

### Crash Counting
The environment uses `_count_crashes_filesystem()` to count files in `output/default/crashes/` rather than relying solely on `fuzzer_stats`. This is more reliable for real-time feedback.

## Integration with RL Agent
This environment is compatible with **Stable Baselines 3** (SB3). The `NeuroFuzzOrchestrator` wraps it in a `Monitor` wrapper and uses it to train the PPO agent.
